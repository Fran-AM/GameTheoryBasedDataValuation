\chapter{Marco teórico}
\justifying

\section*{Introducción}

aaa

\section{Teoría de juegos}
\subsection*{Definición y conceptos básicos}

La \emph{teoría de juegos} puede ser entendida como la rama de las
matemáticas que analiza situaciones en las que el resultado para
cada jugador o participante depende no solo de sus propias
decisiones, sino también de las tomadas por otros
jugadores. Estas situaciones se conocen como \emph{juegos}.

\begin{definition}
  Un \emph{juego} es una interacción entre jugadores
  racionales\footnote{Entendemos la racionalidad como el
  hecho de que cada jugador intenta maximizar su propio
  beneficio.}, mutuamente conscientes, en la que las
  decisiones de un jugador impactan en las ganancias de
  otros. Un juego se define por:
  \begin{itemize}
    \item Los jugadores que intervienen. Cada \emph{jugador}
      es un agente que tiene a su disposición diversas
      estrategias basadas en las posibles recompensas que
      podría recibir.

    \item Las estrategias disponibles para cada jugador.
      Una \emph{estrategia} es un plan de acción que un jugador
      puede adoptar dentro de un juego. Esta estrategia dicta las
      acciones que tomará en cada situación posible que se presente
      en el juego.

    \item Las ganancias de cada jugador en función de los
      resultados. Las \emph{ganancias} son representaciones de
      las motivaciones de los jugadores, pudiendo representar
      beneficios, cantidades, o simplemente reflejar la conveniencia
      de los diferentes desenlaces.
  \end{itemize}
\end{definition}

En el contexto de teoría de juegos aplicada a la valoración de datos,
nos centraremos en juegos cooperativos.

\begin{definition}
  % Ojito con esta definición.
  Un \emph{juego cooperativo} es aquel en el que los jugadores
  pueden comunicarse y negociar con el fin de establecer acuerdos
  vinculantes.
\end{definition}

Los acuerdos mencionados se denominan \emph{coaliciones}.
Se trata de grupos de jugadores que eligen actuar juntos para
lograr un objetivo común. Las coaliciones pueden
variar desde la compuesta por todos los jugadores hasta la que
incluye a un único jugador.


Un juego cooperativo queda completamente definido por el conjunto
de sus jugadores $N$ y por su función característica. Esta función
es el nexo entre la formación de coaliciones y los beneficios
obtenidos por los jugadores, ya que asigna a cada coalición posible
un beneficio que puede lograr.

\begin{definition}
  Una \emph{función característica} $v$ es una función
  \begin{align*}
    v&:2^N \longrightarrow \mathbb{R}\\
    S&\longmapsto v(S).
  \end{align*}
  Asignando a cada posible coalición la máxima
  utilidad que los jugadores de $S$ pueden obtener,
  independientemente de lo que haga el resto de
  jugadores.
\end{definition}


Una vez formadas las coaliciones y determinadas las ganancias,
surge la pregunta: ¿cómo se dividen las ganancias entre los
miembros de la coalición? Varias respuestas a esta pregunta
han surgido en la literatura, pero el valor de
Shapley es uno de los conceptos más prominentes.

\subsection{El valor de Shapley}

El \emph{Valor de Shapley} es un concepto de teoría de juegos
que asigna de manera equitativa las ganancias
entre los miembros de una coalición. Propuesto
por Lloyd Shapley en 1952 \cite{shapleyValue},y 
se fundamenta en los siguientes axiomas:

\begin{itemize}
  \item Simetría. Si dos jugadores son simétricos, es decir,
  si su contribución a cualquier coalición es la misma,
  entonces presentan el mismo valor.
  $$\text{Si } \forall S \subseteq N \setminus \{i, j\},\ 
  v(S \cup \{i\}) = v(S \cup \{j\}) \implies \phi(i;v) =
  \phi(j;v).$$

  \item Eficiencia. El valor total producido por la coalición
  conformada por todos los jugadores se distribuye entre los
  jugadores. Es decir $v(N)=\sum_{i\in N}\phi(i;v)$.

  \item Linealidad. Dados dos juegos $v$ y $w$, el valor del
  juego $v+w$ es la suma de los valores de cada juego. Es decir,
  $\phi(i;v+w)=\phi(i;v)+\phi(i;w)$.

  \item Jugador nulo. Los jugadores que no aportan a ninguna
  coalición tendrán valor nulo. Es decir,
  $$\text{Si } \forall S \subseteq N \setminus \{i\},\ 
  v(S \cup \{i\}) = v(S) \implies \phi(i;v) = 0.$$
\end{itemize}

El siguiente teorema, extraído de \cite{shapleyValue} prueba
que se trata del único método de valoración de datos satisfaciendo
estos axiomas.

\begin{theorem}
  Existe una única función $\phi$ satisfaciendo los axiomas
  de simetría, eficiencia, linealidad y jugador nulo, y viene
  dada por la fórmula:
  $$
  \phi(i;v)=\sum_{S\subseteq N}
  \frac{|S|-1!(|N|-|S|)!}{|N|!}(v(S)-v(S \setminus \{i\}))
  $$
\end{theorem}

\begin{proof}
  La demostración detallada se encuentra en \cite{shapleyValue}.
\end{proof}

\begin{proposition}
  El valor de Shapley puede ser expresado como:
  $$
    \phi(i;v)=\frac{1}{n}\sum_{k=1}^n \binom{n-1}{k-1}^{-1} 
    \sum_{S \subseteq N \setminus \{i \},||S||=k-1}(v(S\cup\{i\})-v(S))
  $$
\end{proposition}

\begin{proof}
  Pendiente.
\end{proof}

Como observación final, es importante señalar que se toman en cuenta
todas las coaliciones que no incluyen al jugador en cuestión.
Para cada una de esas coaliciones, se determina la
\emph{contribución marginal} del jugador, la cual
representa la diferencia entre la ganancia de la coalición
con y sin el jugador.
El Valor de Shapley, en última instancia, se calcula como
el promedio de todas estas contribuciones marginales.


\subsection{Semivalores}

Los semivalores son una generalización del valor de Shapley,
que surgen al relajar el axioma de eficiencia. Mientras que
el Valor de Shapley garantiza que la suma de los valores
individuales de los jugadores sea igual a la utilidad total
generada por la coalición total, los semivalores no imponen
esta restricción. Así, permiten una variedad más amplia de
métodos de valoración en el estudio de juegos cooperativos,
siendo especialmente útiles en escenarios donde no es necesario
o deseado que la totalidad de la recompensa sea distribuida.

La clave de los semivalores es que asignan pesos a las
coaliciones basándose en el tamaño de estas. Estos pesos
son utilizados para calcular la contribución marginal promedio
de cada jugador.

A diferencia del valor de Shapley, que es único dada su
definición basada en axiomas \cite{shapleyValue}, hay múltiples
semivalores posibles, dependiendo de cómo se determinen
los pesos. Esta variabilidad en los semivalores queda
formalizada en el siguiente teorema:

\begin{theorem}{Representación de semivalores \cite{Dubey2}.}
  
  Una función $\phi$ es un semivalor, si y solo
  si, existe una función peso $w:[n] \rightarrow
  \mathbb{R}$ tal que
  $$\sum_{k=1}^n \binom{n-1}{k-1}w(k) = n$$,
  que permite representar $\phi$ mediante la expresión:
  $$
  \phi(i;v) = \sum_{k=1}^n \frac{w(k)}{n}
  \sum_{S \subseteq N \setminus \{i \},||S||=k-1}
  (v(S\cup \{i\} - v(S)))
  $$
\end{theorem}

Es relevante notar que el valor de Shapley es un caso
particular de semivalor, donde los pesos son determinados
por la fórmula: ($w_{Shapley} = \binom{n-1}{k-1}^{-1}$).


\subsection{El valor de Banzhaf}

El \emph{Valor de Banzhaf}, también denominado índice de poder
de Banzhaf\cite{banzhaf}, es una métrica en la teoría de juegos
cooperativos que busca cuantificar el poder e influencia de un
jugador dentro de una coalición.
Este índice fue introducido por John F. Banzhaf III en 1965,
con la finalidad de ofrecer una herramienta analítica que
pudiese determinar el poder de influencia de un jugador,
especialmente en escenarios de votación ponderada.

\begin{definition}
  El \emph{valor de Banzhaf} de un jugador $i$ en un juego
  cooperativo $v$ viene dado por la expresión:
  \begin{equation}
    \label{banzhafFormula}
    \phi_{Banzhaf}(i;v) = \frac{1}{2^{n-1}} \sum_{S
    \subseteq N \setminus \{i\}} [v(S \cup \{i\}) - v(S)]
  \end{equation}
\end{definition}

Para comprender mejor la esencia y la utilidad del valor de
Banzhaf, es fundamental familiarizarse con ciertos conceptos
relacionados:

\begin{itemize}
  \item \emph{Sistema de votación ponderado}: Es un sistema de
  votación en el que cada jugador tiene un peso o poder de voto
  particular. Para que una propuesta sea aprobada, la suma de los
  pesos de los jugadores que votan a favor debe superar un
  umbral o cuota establecida.

  \item \emph{Jugador pivote}: Se considera que un jugador actúa
  como pivote si, al modificar su voto de negativo a positivo,
  la propuesta es aprobada. Sin embargo, si se abstuviera o
  mantuviera su voto en contra, la propuesta sería rechazada.

  \item \emph{Índice de poder de Banzhaf} Este índice mide la
  frecuencia con la que un jugador se convierte en pivote.
  Es importante destacar que el poder de un jugador no siempre
  es directamente proporcional a su peso en la votación.
\end{itemize}

A pesar de sus similitudes con el valor de Shapley, el índice
de Banzhaf se diferencia en su enfoque y en la forma de asignar
poder a los jugadores.
Mientras que el valor de Shapley se basa en contribuciones
marginales promediadas, el índice de Banzhaf se enfoca en la
capacidad de un jugador de influir en el resultado final de
una votación. Específicamente, es un semivalor
con un peso asociado dado por $w_{Banzhaf} = \frac{1}{2^{n-1}}$.


% A lo mejor está bien añadirlo
% En escenarios prácticos, el valor de Banzhaf ha demostrado ser una herramienta
% esencial para analizar sistemas de votación, especialmente en estructuras como
% los sistemas electorales y las decisiones corporativas. Al permitir una
% comprensión más profunda del poder real que cada jugador posee,
% independientemente de su peso nominal, las partes interesadas pueden tomar
% decisiones más informadas y equitativas.

\newpage
\section{Valoración de datos}

Hoy en día, el dato representa uno de los recursos más
valiosos en el mundo para negocios, gobiernos y particulares.
La toma de decisiones basada en datos está presente en
casi todos los ámbitos de la sociedad, desde la medicina
predictiva hasta la publicidad personalizada. Debido a
esto, la habilidad para determinar el valor de un dato
se ha vuelto indispensable. Es aquí donde entra
en juego la valoración de datos.

\

Cuando nos referimos al valor de un dato, es esencial
comprender que dicho valor no es unidimensional. Un dato
puede ser valorado desde diferentes perspectivas y
categorizado basado en diversas cualidades:

\begin{enumerate}
  \item \textbf{Valor Intrínsico vs. Extrínseco}:
  \begin{itemize}
    \item \textbf{Valor intrínseco}: Se refiere al valor
    inherente al propio dato, basado en su precisión y calidad.
    Este valor es independiente del uso que se le dé al dato.

    \item \textbf{Valor extrínseco}: Se corresponde al valor
    que se le atribuye al dato en función de su uso.
    Depende, pues, del contexto en el que se use el dato.
  \end{itemize}

  \item \textbf{Valor Directo vs. Indirecto}:
  \begin{itemize}
    \item \textbf{Valor directo}: Alude al beneficio
    inmediato que se obtiene de un dato, como podría ser
    al venderlo.

    \item \textbf{Valor indirecto}: Se refiere al
    beneficio derivado del uso estratégico del dato.
  \end{itemize}
\end{enumerate}

En este trabajo nos centraremos en estudiar el valor extrínseco
e indirecto de los datos. Esta investigación nos permitirá,
posteriormente, discernir el valor directo de los datos y
detectar posibles problemas en su valor intrínseco.

A pesar de que la valoración de datos es un concepto
multifacético que depende de varios factores, nos
ajustaremos al enfoque propuesto por diversas
fuentes\cite{dataShapley,betaShapley}, el cual se compone
de tres elementos esenciales:

\begin{enumerate}
  \item  Denominaremos $N$ al \textbf{conjunto prefijado de datos
  de entrenamiento}, siendo $N = \{ (x_i, y_i) \}_1^n$.
  Aquí, $x_i$ hace referencia a las características del dato
  $i$-ésimo, e $y_i$ a su categoría en problemas de
  clasificación o su valor en problemas de regresión.
  
  \item El \textbf{algoritmo de aprendizaje}  $\mathcal{A}$,
  será tratado como una caja negra que toma un conjunto de
  entrenamiento $N$ y genera un predictor $f$.
  
  \item La \textbf{función de utilidad} $v$ es una aplicación
  que asigna a cada subconjunto de $N$ un valor, reflejando
  la utilidad de ese subconjunto. Para problemas de clasificación,
  la opción común para $v$ es la precisión del modelo
  entrenado con el subconjunto dado, es decir
  $v(S) = acc (\mathcal{A} (S))$. Sin pérdida de generalidad
  asumiremos a lo largo del documento que $v (S) \in [0, 1]$
  para cualquier $S \subseteq N$.
\end{enumerate}

Por lo tanto, podemos concebir la valoración de datos como el
proceso de asignar un valor a cada dato del conjunto $N$,
reflejando su contribución en el entrenamiento del modelo.
Cada uno de estos valores estará determinado por $N,
\mathcal{A}$ y $v$, pero por simplicidad, lo expresaremos
como $\phi(i ; v)$. A estas puntuaciones se les denomina
\textit{data values}.

Pasamos a tratar las nociones principales en cuanto a valoración
de datos.

\subsection{Métricas de valoración de datos}

\subsubsection{LOO Error}

El método más sencillo para valorar datos consiste en medir la
contribución de un punto individual al desempeño global del
conjunto de entrenamiento:
\[
  \phi_{{loo}} (i ; v) = v (N) - v (N \setminus i).
\]
Este método es conocido como \textit{leave-one-out}(LOO).
Para calcular el valor exacto de los valores LOO para
un conjunto de entrenamiento de tamaño $N$, sería
necesario reentrenar el modelo $N$ veces. Este procedimiento
resulta poco práctica cuando el tamaño del conjunto de datos
es considerablemente grande, como se puede observar en estudios
tales como \cite{looFuck}

\subsubsection{Data Shapley}
En la sección citarseccióndelvalorDeShapley se introdujo 

En {\cite{shapleyValue}} definen los métodos de valuación equitativa
como sigue.

\begin{definition}
  Un método de evaluación será equitativo si cumple las siguientes
  condiciones:
  \begin{enumerate}
    \item Linealidad: Dadas métricas de error $V$ y $W$, constantes
    
    \item Jugador nulo: Si $\forall S \subseteq N \setminus
    \{ i \}$, $V (S) = V (S \cup \{ i \})$ entonces $\phi_i = 0$.
    
    \item Simetría: Fijados $i, j \in N$, si para todo $S \subseteq N
    \setminus \{ i, j \}$, se tiene que $V (S \cup \{ i \}) = V (S \cup j)$
    entonces $\phi_i = \phi_j$.
    
    \item Esta hay que escribirla bien.
  \end{enumerate}
\end{definition}

La siguiente proposición es la que da nombre a ...

\begin{proposition}
  Cualquier $\phi (N, \mathcal{A}, V)$ que satisfaga las condiciones
  anteriores será de la forma
  \[
  \phi_i = C \sum_{S \subseteq N \setminus \{ i \}}
  \frac{V (S \cup \{ i\}) - V (S)}{\binom{n - 1}{| S |}} .
  \]
  Dónde el sumatorio contempla todos los subconjuntos de $N$ que no
  contienen a $i$ y $C$ es una constante arbitraria. Llamaremos a $\phi_i$ el
  Valor de Shapley asociado al dato $i$.
\end{proposition}

\begin{proof}
  Se puede consultar en {\cite{shapleyValue}}.
\end{proof}


\subsubsection{Beta Shapley}
En \cite{betaShapley} proponen un semivalor concreto... 
En este artículo se propone utilizar la función $\beta$ con
un par de parámetros positivos $(\alpha,\beta)$ para definir
un semivalor, que vendrá dado por la siguiente función
de peso
\[
  jeje
\]

We propose to use $\psi$ semi(z*; U, D, w(n) $\alpha, \beta$ ) and call it Beta($\alpha, \beta$)-Shapley
value. The pair of hyperparameters ($\alpha, \beta$) decides the weight distribution on [n].
For instance, when ($\alpha, \beta$) = (1, 1), the normalized weight w(n) $\alpha, \beta$ (j) : =
(n-1 j-1 )w(n) $\alpha, \beta$ (j) = 1 for all j in [n], giving the uniform weight on
marginal contributions , i.e., Beta(1,1)-Shapley $\psi$semi(z*; U, D, w(n) 1,1 ) is
exactly the original data Shapley. Figure 3 shows various weight distributions
for different pairs of ($\alpha, \beta$). For simplicity, we fix one of the hyperparameter
to be one. When $\alpha$ \(\geq \) $\beta$ = 1, the normalized weight assigns large weights
on the small cardinality and remove noise from the large cardinality. Conversely,
Beta(1,$\beta$) puts more weights on large cardinality and it approaches to the LOO
as $\beta$ increases.

\subsubsection{Banzhaf}
Este concepto será el central a lo largo de nuestro trabajo.
Se define igual que su precursor, el valor de Banzhaf descrito
en la sección... 

El interés particular de este concepto es que será, de todos
los semivalores el que alcanza un mayor \textit{safety margin},
concepto que introduciremos en la sección...


\newpage
\section{Midiendo la robustez}

En diversas situaciones, como la selección de datos, el orden
de los \textit{data values} es lo que aporta valor\cite{betaShapley}.
Un ejemplo podría ser el filtrado de datos de baja calidad.
El escenario ideal, sería aquel en el que incluso
estando perturbada la función de utilidad, se preserva
el mismo orden de \textit{data values}.

En este contexto, la robustez alude a la resistencia
de los métodos de valoración de datos ante perturbaciones
o ruido. Del mismo modo que un modelo de aprendizaje robusto
debería resistir entradas ruidosas, un método de valoración de
datos robusto debería conservar el orden de los \textit{data values} a pesar
del ruido intrínseco de los algoritmos
de aprendizaje automático.

\

Ahora, vamos a establecer los conceptos requeridos para
formalizar y medir la robustez. Recordemos que un semivalor
está determinado por su función de peso $w$. Así, definimos
la diferencia escalada como:


\begin{definition}
  Sean $i$ y $j$ puntos de $N$. La diferencia
  entre los semivalores $\phi(i;v)$ y $\phi(j;v)$
  se define como:
  \begin{align*}
    D_{i,j}(v,w)&:= n(\phi_w(i;v)-\phi_w(j;v))\\
    &=\sum_{k=1}^{n-1} (w(k)+w(k+1)) \binom{n-2}{k-1}
    \Delta_{i,j}^k(v).
  \end{align*}
  Donde $\Delta_{i,j}^k(v):=\binom{n-2}{k-1}^{-1} \sum_{|S|=k-1,
  S\subseteq N \setminus \{i,j\}} (v(S \cup i)-v(S \cup j))$,
  representa la distinguibilidad promedio entre $i$ y $j$ en
  subconjuntos de tamaño $k$ usando una función de utilidad
  sin ruido $v$.
\end{definition}

Consideremos $\hat{v}$ un estimador de $v$. Sabemos que
$\hat{v}$ y $v$ generarán diferentes \textit{data values}
para un par de puntos $i$ y $j$ si, y solo si,
$D_{i,j}(v,w)D_{i,j}(\hat{v},w) \leq 0$.

Se podría pensar inicialmente en definir
la robustez de un semivalor como la menor cantidad de ruido
$||\hat{v}-v||$ que alteraría el orden de los
\textit{data values}. Sin embargo, una definición
así dependería de la función de utilidad original $v$.
Si la función original $v$ no es capaz de diferenciar
dos puntos $i$ y $j$ ($\Lambda_{i,j}^{(k)}(v)\simeq 0$, para
todo $k=1,\dots,n-1$), entonces $D_{i,j}(v,w)$ será
casi 0, y cualquier mínima perturbación podría 
modificar el orden entre $\phi(i;v)$ y $\phi(j;v)$. 

Por ello, para definir de formar razonable la
robustez de un semivalor, debemos considerar solo las
funciones de utilidad que sean capaces de distinguir
entre $i$ y $j$.

\begin{definition}{Distinguibilidad}
  Diremos que un par de puntos $(i,j)$ son $\tau$-distinguibles por
  la función de utilidad $v$ si, y solo si, $\Lambda_{i,j}^{(k)}(v) \geq \tau$
  para todo $k \in \{1,\dots,n-1\}$.
\end{definition}

Sea ahora $\mathcal{V}_{i,j}^{(k)}$ el conjunto de todas las funciones
de utilidad $v$ que son capaces de $\tau$-distinguir $(i,j)$. 
Usando la definción anterior, podemos caracterizar la robustez de
un semivalor mediante su \textit{safety margin}, que representa la
menor cantidad de ruido $||\hat{v} - v||$ que, al añadirse,
invertiría el orden de los \textit{data values} de al menos un
par de puntos $(i,j)$, para al menos una función de utilidad $v \in
\mathcal{V}_{i,j}^{(k)}$.

\begin{definition}{Safety margin}
  Dado $\tau > 0$, definimos el \textit{safety margin} de un
  semivalor para un par de puntos $(i,j)$ como:
  \begin{equation*}
    \text{Safe}_{i,j}^{(k)}(\tau;w):=\min_{v \in \mathcal{V}_{i,j}^{(\tau)}}
    \min_{\hat{v} \in \{\hat{v}:D_{i,j}(v;w)D_{i,j}(\hat{v};w)\leq 0\}}
    ||\hat{v} - v||.
  \end{equation*}
  El \textit{safety margin} de un semivalor es:
  \begin{equation*}
    \text{Safe}(\tau;w):=\min_{i,j \in N, i \neq j} \text{Safe}_{i,j}(\tau;w).
  \end{equation*}
\end{definition}

La intuición detrás del \textit{safety margin} es que muestra
la máxima cantidad de ruido que puede ser añadida a un semivalor sin que
se altere el orden de los \textit{data values} de ningún par de
puntos que fuera distinguible por la función de utilidad original.


% Corregido hasta aquí por GPT.
\subsection{Robustez del valor de Banzhaf}
Los resultados aquí mostrados pertenecen a la sección 4
de \cite{dataBanzhaf}.

\begin{theorem}
  Para cualquier $\tau > 0$, el valor de Banzhaf
  alcanza el mayor \textit{safety margin} ,
  \[
  \text{Safe}(\tau;w_{Banzhaf})=\frac{\tau}{2^{\frac{n}{2}-1}}.  
  \]
  De entre todos los semivalores.
\end{theorem}

\begin{proof}
  Consultar...
\end{proof}

Intuitivamente, este resultado se debe a cómo los
semivalores asignan diferentes pesos en función del
tamaño de los subconjuntos evaluados. Así, es posible
construir una perturbación de la función de utilidad
que maximice la influencia sobre el semivalor
correspondiente, introduciendo ruido en los subconjuntos
con mayor peso asignado. De ahí que la estrategia
óptima para robustecer sea asignar pesos uniformes a
todos los subconjuntos, tal como lo hace el valor de Banzhaf.

Además, se puede demostrar que el valor de Banzhaf es
el semivalor más robusto en el sentido de que el ruido
de la utilidad afecta mínimamente a los cambios en los
\textit{data values}. En concreto, el valor de
Banzhaf alcanza la menor constante de Lipschitz $L$
tal que $||\phi(v)-\phi(\hat{v})|| \leq L||v-\hat{v}||$
para todos los posibles pares de funciones de utilidad
$v$ y $\hat{v}$.

\begin{theorem}
  El valor de Banzhaf, con $w(k) = \frac{n}{2^{n-1}}$, logra
  la menor constante de Lipschitz,
  $L = \frac{1}{2^{\frac{n}{2}-1}}$ de entre todos los semivalores.
\end{theorem}

\begin{proof}
  Consultar apéndice C.4 de \cite{dataBanzhaf}.
\end{proof}


\subsection{Estimación eficiente}

Dado que es prácticamente imposible calcular los
\textit{data values} exactos en métodos de valoración
de datos basados en semivalores, debido a la necesidad
de un número exponencial de evaluaciones de la función
de utilidad, se debe recurrir a métodos de aproximación.

A continuación, introducimos el concepto de error
asociado a un estimador.

\begin{definition}
  Un estimador de un semivalor $\hat{\phi}$ es
  una $(\epsilon,\delta)$-aproximación del semivalor $\phi$ 
  en norma $l_p$ si, y solo si,
  \[
  P_{\hat{\phi}}[||\hat{\phi}-\phi)||_p\leq \epsilon] \geq 1-\delta.
  \]
  Donde la aleatoriedad se da en la construcción del estimador.
\end{definition}

\subsection{Estimador Simple de Montecarlo}
El valor de Banzhaf %\ref{banzhafFormula}
puede ser reformulado como:
\begin{equation}
  \label{simpleMontecarlo}
  \phi_{Banzhaf}(i;v) = \mathcal{E}_{S \sim \text{Unif}
  (2^{N\setminus i})} [v(S \cup \{i\})-v(S)].
\end{equation}

A partir de esto, un método de Montecarlo directo
para estimar $\phi_{Banzhaf}(i;v)$ consistiría en
generar muestras uniformes de $\mathcal{S}_i
\subset 2^{N \setminus \{i\}}$ y calcular:
\begin{equation}
  \hat{\phi}_{MC}(i;v) = \frac{1}{|\mathcal{S}_i|}\sum_{S \in
  \mathcal{S}_i} [v(S_j \cup \{i\})-v(S_j)].
\end{equation}

Al repetir este proceso para cada punto $i \in N$, obtendremos
el estimador $\hat{\phi}_{MC} = [\hat{\phi}_{MC}(1),\dots,
\hat{\phi}_{MC}(n)]$.


\begin{theorem}
  El estimador de Montecarlo simple $\hat{\phi}_{MC}$
  es una $(\epsilon,\delta)$-aproximación de $\phi_{Banzhaf}$
  en norma $l_p$ con $\mathcal{O}(\frac{n^2}{\epsilon^2}
  \log(\frac{n}{\delta}))$ evaluaciones de $v$, y
  $\mathcal{O}(\frac{n}{\epsilon^2}
  \log{\frac{n}{\delta}})$ evaluaciones de $v$ en la norma
  $l_{\inf}$.
\end{theorem}

\begin{proof}
  Consultar apéndice C.1.2 de \cite{dataBanzhaf}.
\end{proof}

El método anterior podría mejorarse en eficiencia,
dado que cada muestra $S \in \mathcal{S}_i$ generada
solo contribuye a la estimación de
$\hat{\phi}_{Banzhaf}(i;v)$. Esto introduce un
factor de $n$ en la complejidad, ya que es necesario
generar un mismo número de muestras para cada dato.

Es en este contexto donde surge el concepto del
estimador de máxima reutilización (MSR), propuesto
en \cite{dataBanzhaf}. La idea es explotar la
linealidad de la esperanza, de forma que:

\begin{equation}
  \label{maximumSampleReuse}
  \phi_{Banzhaf}(i;v) = \mathcal{E}_{S \sim \text{Unif}(2^{N\setminus i})}
  [v(S \cup \{i\})] - 
  \mathcal{E}_{S \sim \text{Unif}(2^{N\setminus i})}
  [v(S)].
\end{equation}

Tomemos como ejemplo un conjunto de $m$ muestras
$\mathcal{S} = \{S_1,\dots,S_m\}$, generado de manera
uniforme. Para cada $i \in N$, podemos clasificar las
muestras de $\mathcal{S}$ en dos categorías:

\begin{itemize}
  \item $\mathcal{S}_{\ni i}$: el conjunto de muestras
  que contienen el dato $i$, es decir,
  $\mathcal{S}_{\ni i} = \{S \in \mathcal{S}: i \in S\}$.
  \item $\mathcal{S}_{\not \ni  i}$: el conjunto de muestras
  queno contienen el dato $i$, esto es,
  $\mathcal{S}_{\not \ni  i} = \{S \in \mathcal{S}: i \not
  \in S\}$.
\end{itemize}

Así, para cada jugador, diferenciamos entre las muestras
que incluyen a dicho jugador y las que no.

Utilizando esta clasificación, podemos estimar
$\phi_{Banzhaf}(i;v)$ de la siguiente manera:

\begin{equation}
  \hat{\phi}_{MSR}(i;v) = \frac{1}{|\mathcal{S}_{\ni i}|}
  \sum_{S \in \mathcal{S}_{\ni i}} v(S) -
  \frac{1}{|\mathcal{S}_{\not \ni i}|}
  \sum_{S \in \mathcal{S}_{\not \ni i}} v(S).
\end{equation}

A este método le denominamos \textit{estimador de máxima
reutilización} (MSR).
\begin{theorem}
  $\hat{\phi}_{MSR}$ es una $(\epsilon,\delta)$-aproximación
  de $\phi_{Banzhaf}$ en norma $l_p$ con $\mathcal{O}
  (\frac{n}{\epsilon^2} \log(\frac{n}{\delta}))$ evaluaciones
  de $v$, y $\mathcal{O}(\frac{1}{\epsilon^2}
  \log{\frac{n}{\delta}})$ evaluaciones de $v$ en la norma
  $l_{\inf}$.
\end{theorem}

\begin{proof}
  Consultar apéndice C.1.2 de \cite{dataBanzhaf}.
\end{proof}

Uno podría cuestionarse: ¿por qué se opta por el valor
de Banzhaf en lugar de otro semivalor? La razón radica
en que el valor de Banzhaf es el único semivalor
que posibilita la implementación del algoritmo MSR.
Como puede verse en el apéndice C.2 de \cite{dataBanzhaf}.

Aunque el MSR destaca por ser superior al método de
Montecarlo simple, surge una pregunta válida:
¿es verdaderamente eficiente el MSR?

La respuesta la encontramos en el siguiente teorema:
\begin{theorem}
  Todo estimador aleatorio del valor de Banzhaf
  que sea una $(\epsilon,\delta)$-aproximación en norma
  $l_{\inf}$ con $\delta \in (0,\frac{1}{2})$ requiere
  al menos $\Omega(\frac{1}{\epsilon})$.
\end{theorem}

Como hemos visto anteriormente el algoritmo MSR presenta
una complejidad de $\mathcal{O}(\frac{1}{\epsilon^2}
\log(\frac{n}{\delta}))$ en la norma $l_{\inf}$.
Lo que quiere decir que se aleja de la optimalidad
en un factor de $\mathcal{O}(\frac{1}{\epsilon}
\log(\frac{n}{\delta}))$.