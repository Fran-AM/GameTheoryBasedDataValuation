\chapter{Análisis Experimental}

\section*{Introducción}
En este capítulo, realizamos un estudio comparativo de
diferentes métodos de valoración de datos, centrándonos
en su eficacia en dos tareas: la detección de datos mal
etiquetados y el entrenamiento ponderado. Nuestra motivación
se basa en contrastar la información existente en trabajos
como \cite{dataShapley, betaShapley} y especialmente
\cite{dataBanzhaf}, en el que se afirma que
\textit{data Banzhaf} es el método de valoración de datos
que mejor se comporta en estas tareas en las que la función
de utilidad $acc(\mathcal{A}(D))$ presenta el ruido inherente
al método del gradiente estocástico usado para el entrenamiento
de los modelos.

\

Buscamos comprender cómo los valores de datos pueden impactar el
entrenamiento de modelos y, en consecuencia, las
decisiones derivadas de estos. A lo largo del capítulo,
ofrecemos detalles sobre los experimentos llevados a cabo,
los \textit{datasets} empleados y las herramientas
esenciales que respaldaron nuestra investigación.
Todos el código desarrollado en este trabajo puede
ser consultado en repoGit.

\newpage

\section{Metodología}
\subsection{Datasets}

Los \textit{datasets} utilizados en los experimentos están
listados en la tabla \ref{tab:datasets}. Estos conjuntos
son comúnmente empleados en la literatura como
\textit{benchmarks} para estudios similares al nuestro
\cite{dataBanzhaf}.

Las características de todos estos \textit{datasets} son
similares. Contienen datos etiquetados, con dos clases
en la variable objetivo. En cuanto al preprocesado de estos,
se destaca que se llevó a cabo un
preprocesado en todos ellos para equilibrar
el número de muestras de cada una de las clases.
% Hay que hacer referencia a que hay codigo en el apéndice.
En el apéndice \ref{ap:apendice2} puede verse el código..


\begin{table}[ht]
    \centering
    \resizebox{0.6\columnwidth}{!}{%
    \begin{tabular}{lc}
        \hline
        \multicolumn{1}{c}{\textbf{Dataset}} & \textbf{Source}                                                       \\ \hline
        \rowcolor[HTML]{FFFFFF} 
        Click                                & \href{https://www.openml.org/d/1218}{https://www.openml.org/d/1218}   \\
        \rowcolor[HTML]{FFFFFF} 
        Phoneme                              & \href{https://www.openml.org/d/1489}{https://www.openml.org/d/1489}   \\
        Wind                                 & \href{https://www.openml.org/d/847}{https://www.openml.org/d/847}     \\
        CPU                                  & \href{https://www.openml.org/d/761}{https://www.openml.org/d/761}     \\
        2DPlanes                             & \href{https://www.openml.org/d/727}{https://www.openml.org/d/727}     \\ \hline
    \end{tabular}%
    }
    \caption{Datasets usados en los experimentos}
    \label{tab:datasets}
\end{table}


% \begin{lstlisting}
%     def undersamp_equilibration(
%         data: np.ndarray,
%         target: np.ndarray
%     ) -> (np.ndarray, np.ndarray):
%     """
%     Equilibrate the classes of a dataset with
%     two classes in the target variable,
%     using undersampling.

%     Args:
%         df (pd.DataFrame): The dataset.

%     Returns:
%         pd.DataFrame: The balanced dataset.
%     """
%     unique_targets, counts = np.unique(target, return_counts=True)
%     min_count = counts.min()
    
%     balanced_indices = np.concatenate([
%         np.random.choice(np.where(target == class_label)[0], min_count, replace=False)
%         for class_label in unique_targets
%     ])
    
%     # Use the indices to extract the corresponding rows from data and target
%     balanced_data = data[balanced_indices]
%     balanced_target = target[balanced_indices]
    
%     return balanced_data, balanced_target
% \end{lstlisting}

\subsection{Herramientas}

Entre las herramientas utilizadas, es relevante
destacar \textit{pyDVL}, una librería de
\href{https://www.appliedai.de/en/}{appliedAI}, y
\textit{DVC}, una herramienta de control de versiones
específica para proyectos de ciencia de datos.

\subsubsection*{pyDVL}

\href{https://aai-institute.github.io/pyDVL/0.7.0/}{pyDVL},
actualmente en su versión \texttt{0.7.0}, es una
librería que engloba diversos algoritmos enfocados al cálculo
de \textit{data values}, estando la mayoría de ellos basados en
la teoría de juegos cooperativos. Con \textit{pyDVL}, es posible
calcular de manera sencilla y automática los \textit{data values}
de un \textit{dataset} determinado, empleando diferentes métodos de
valoración, técnicas de muestreo y estimadores, entre otros. 

El proceso de cálculo de valores de datos en
\textit{pyDVL} se divide en tres pasos esenciales:
\begin{enumerate}
    \item Construcción del dataset de pyDVL a
    partir de tus datos.
    \item Creación de la utilidad, un concepto abstracto
    que interrelaciona el modelo a usar, el dataset y la
    métrica de error.
    \item Cálculo de los valores de datos utilizando el
    método seleccionado.
\end{enumerate}

% Aquí puedes añadir ejemplos de uso de funcionalidades de pyDVL
% \textbf{Ejemplo de uso de pyDVL:}
% \begin{verbatim}
% Código Python de un ejemplo de cómo usar pyDVL...
% \end{verbatim}

\subsection*{DVC}

\textit{DVC}, acrónimo de Data Version Control,
es una herramienta concebida para proyectos de ciencia de
datos, que se integra perfectamente con herramientas de
ingeniería de software ya consolidadas como Git. Se
desarrolla con el propósito deayudar a equipos de machine learning
en la gestión de grandes volúmenes de datos, asegurar la
reproducibilidad de los proyectos y fortalecer el
trabajo colaborativo. \textit{DVC} es compatible con
cualquier terminal y también puede ser invocado como
una biblioteca de Python. Su central  es brindar una experiencia
al estilo Git para estructurar datos, modelos y experimentos en
proyectos de Ciencia de Datos y Aprendizaje Automático. Para
información más detallada, consultar
la \href{https://dvc.org/doc/user-guide}{Documentación de oficial}.


\section{Detección de datos mal etiquetados}

Investigamos la habilidad de distintos métodos de
valoración de datos para identificar puntos mal
etiquetados en escenarios con funciones de utilidad ruidosas.
Esta capacidad de detectar y corregir puntos mal etiquetados
es esencial para mejorar el rendimiento de los modelos de
machine learning y reducir el tiempo de entrenamiento de
los mismos.

\
Para llevar a cabo este experimento generamos muestras
con etiquetas intercambiadas invirtiendo las
etiquetas del 10\% de los puntos de datos de entrenamiento.
A la hora de la evaluación de esta tarea, un punto de
datos se considera incorrectamente etiquetado si su
\textit{data value} se encuentra por debajo del percentil
10 en relacióncon todas las puntuaciones de valoración de datos.

\

Utilizaremos un perceptrón multicapa como modelo, que cuenta
con una única capa oculta compuesta por 100 neuronas. La
función de activación será ReLU, y el \textit{learning rate}
inicial se establece en $10^{-2}$. Se usará el optimizador
Adam durante el proceso de entrenamiento. El tamaño de los
\textit{batches} es de 32 datos para todos los
\textit{datasets}. Dado que el proceso de valoración de datos
es muy costoso desde el punto de vista computacional, trabajaremos
con subconjuntos de los \textit{datasets} originales. De cada
conjunto, seleccionaremos 200 muestas al azar para
llevar a cabo el experimento. En relación a los estimadores que
se utilizarán para cada método, aplicaremos Permutation Sampling
en todos los casos.

\section{Entrenamiento ponderado}

Estudiamos ahora cómo se puede aplicar un sistema de
ponderación basado en \textit{data values} para realizar
una suerte de submuestreo de puntos de datos.
\href{https://acortar.link/bclshR}{SGDClassifier} de
\href{https://scikit-learn.org/stable/index.html}{scikit-learn},
en su modalidad de regresión logística.

Como parámetros usaremos regularización estándar $L_2$,
con coeficiente $\alpha= 10^{-4}$. El número máximo
de épocas de entrenamiento será de $10^3$, y tolerancia
de $10^{-3}$ para el criterio de parada.

Para la ponderación, normalizaremos el \textit{data value}
asociado a cada punto de entrenamiento en el rango $[0,1]$.
Durante el proceso de entrenamiento, cada muestra se multiplicará por
su peso correspondiente. Esto significa que los puntos con
valores más altos tendrán una mayor influencia en la regresión
logística. Finalmente, entrenaremos los clasificadores
utilizando los conjuntos de entrenamiento y evaluaremos la
precisión en los conjuntos de test.



